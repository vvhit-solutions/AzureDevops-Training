# Grand Central Platform - Technical Interview Questions

## Repository Analysis Summary

This interview question bank is based on analysis of the Grand Central platform repositories, which implement a sophisticated financial technology infrastructure including:

- **Azure-based Infrastructure**: Multi-layered Terraform deployments with 8-tier architecture
- **Kubernetes GitOps**: ArgoCD-managed applications with multi-source deployments
- **Banking Integrations**: Multiple core banking system connectors (Mambu, Flexcube, FIS, etc.)
- **Multi-tenant Architecture**: Support for multiple financial institutions
- **Enterprise Security**: Azure AD, Key Vault, Kyverno policies, SOPS encryption
- **CI/CD**: GitHub Actions with OIDC, matrix builds, automated terraform planning
- **Observability**: Comprehensive monitoring, testing frameworks, and compliance checks

---

## ðŸ“˜ Basic Questions

### Infrastructure & Terraform

**Q1: Explain the 8-tier deployment architecture used in this platform.**

**Answer:** The platform uses a layered infrastructure deployment approach:
1. **Subscription**: Sets up Azure subscription, policies, and role assignments
2. **SubscriptionInit**: Bootstraps foundational resources (resource groups, event hubs, log analytics)
3. **Services**: Provisions shared services (Azure AD groups, container registries, DNS)
4. **DevOps**: Sets up CI/CD tooling and pipelines
5. **Baseline**: Establishes network and security baseline per environment
6. **Storage**: Deploys storage accounts and messaging infrastructure
7. **Compute**: Provisions servers and cloud resources
8. **Observability**: Adds monitoring and alerting

**Code Example:**
```terraform
# From shared/terraform.tf
terraform {
  required_providers {
    azurerm = { 
      source = "hashicorp/azurerm"
      version = "=4.61.0" 
    }
  }
  backend "azurerm" {
    resource_group_name  = "rg-tfstate"
    storage_account_name = "stgcsharedtfstate"
    container_name       = "tfstate"
    key                  = "terraform-shared.tfstate"
    use_azuread_auth     = true
  }
}
```

**Repository Reference:** [gc-infra/Readme.md](gc-infra/Readme.md#L15-L45)

**Production Importance:** This structured approach ensures consistent, repeatable deployments across environments while maintaining security boundaries and dependency management. It allows for scaling as business units grow and provides clear separation of concerns.

---

**Q2: What is the purpose of the tier configuration files in the bootstrap folder?**

**Answer:** The tier configuration files (`tier_external.json`, `tier_internal.json`, `tier_test.json`) define which organizations/customers belong to different deployment tiers and specify the infrastructure version to use.

**Code Example:**
```json
{
  "version": "0.714.0",
  "orgs": [
    "ever",
    "wb", 
    "alc"
  ]
}
```

**Repository Reference:** [gc-bootstrap-config/bootstrap/tier_external.json](gc-bootstrap-config/bootstrap/tier_external.json)

**Production Importance:** This allows for controlled rollouts of infrastructure changes across different customer tiers, enabling gradual deployment and risk mitigation in a multi-tenant environment.

---

### Kubernetes & ArgoCD

**Q3: Explain the ArgoCD multi-source application pattern used in this platform.**

**Answer:** The platform uses ArgoCD's multi-source feature to deploy applications from multiple repositories:
- One source for configuration (git repository with values files)
- Multiple sources for different Helm charts from Azure Container Registry
- Each connector (party, deposit, loan, payment) can have different versions

**Code Example:**
```yaml
# From gc-mambu.yaml
spec:
  sources:
    - repoURL: git@github.com:bb-ecos-ecos/gc-ecos-applications-live.git
      targetRevision: main
      ref: apps-live
    - repoURL: crecos493.azurecr.io/charts
      chart: gc-mambu-party-connector
      targetRevision: 1.0.10
      helm:
        releaseName: party-v0
        valueFiles:
          - $apps-live/runtimes/dev/values/platform-resource-management/platform-resource-management.yaml
          - $apps-live/runtimes/dev/values/gc-mambu/party-v0.values.yaml
```

**Repository Reference:** [gc-ecos-applications-live/runtimes/dev/apps/gc-mambu.yaml](gc-ecos-applications-live/runtimes/dev/apps/gc-mambu.yaml#L10-L25)

**Production Importance:** This pattern enables independent versioning of application components while maintaining centralized configuration management. It supports gradual rollouts and rollbacks of individual services without affecting the entire application.

---

**Q4: What environments are supported and how are they organized?**

**Answer:** The platform supports four main environments:
- **dev**: Development environment for testing
- **stg**: Staging environment for pre-production validation  
- **test**: Testing environment for QA
- **uat**: User Acceptance Testing environment

Each environment has its own directory structure with apps, secrets, values, istio, and synchub configurations.

**Repository Reference:** [gc-ecos-applications-live/runtimes/](gc-ecos-applications-live/runtimes/)

**Production Importance:** Clear environment separation ensures proper testing workflows, risk mitigation, and compliance with financial industry regulations requiring segregation of duties.

---

## ðŸ“— Intermediate Questions

### Security & Secrets Management

**Q5: How does the platform handle secrets management across environments?**

**Answer:** The platform uses a multi-layered secrets approach:
1. **SOPS (Secrets OPerationS)** for encrypting secrets in Git
2. **Azure Key Vault** for centralized secret storage
3. **Kubernetes secrets** deployed through ArgoCD
4. **RBAC** controls for access management

**Code Example:**
```terraform
# From shared/keyvault.tf
resource "azurerm_key_vault" "kv_shared" {
  location                        = var.location
  name                            = local.shared_keyvault_name
  resource_group_name             = local.shared_resourcegroup
  sku_name                        = "standard"
  tenant_id                       = local.tenant_id
  enabled_for_disk_encryption     = false
  enable_rbac_authorization       = true
  enabled_for_template_deployment = true
  soft_delete_retention_days      = 7
  purge_protection_enabled        = true
}
```

**Repository Reference:** [gc-infra/shared/keyvault.tf](gc-infra/shared/keyvault.tf#L11-L25)

**Production Importance:** Multi-layered secrets management ensures compliance with financial regulations, provides audit trails, and maintains security even if one layer is compromised.

---

**Q6: Explain the Kyverno policy enforcement strategy used in the platform.**

**Answer:** Kyverno is used for Kubernetes policy enforcement with different policies applied based on runtime environment:
- **Dev runtime**: More permissive policies (e.g., allows latest tags)
- **Protected runtimes** (prd/test/stg): Strict policies (e.g., disallow latest tags)

**Code Example:**
```python
# From policy_existence_test.py
class PolicyExistenceTest(BaseTestLifecycle):
    """
    Test that validates Kyverno policies exist based on runtime configuration.
    
    Based on infrastructure deployment patterns:
    - Dev runtime: disallow-latest-tag policy should NOT be deployed
    - Protected runtimes: disallow-latest-tag policy SHOULD be deployed
    """
    
    def pre_deploy(self):
        self.runtime_info = KyvernoRuntimeDetector.should_enforce_policy(
            self.context,
            policy_name="disallow-latest-tag"
        )
```

**Repository Reference:** [gc-infra-config/gc-tests/kyverno/policy_existence_test.py](gc-infra-config/gc-tests/kyverno/policy_existence_test.py#L15-L35)

**Production Importance:** Automated policy enforcement reduces security risks, ensures compliance, and provides consistency across environments while allowing appropriate flexibility for development.

---

### CI/CD & DevOps

**Q7: How does the GitHub Actions workflow implement secure Azure authentication?**

**Answer:** The platform uses OpenID Connect (OIDC) for secure, keyless authentication to Azure:
- **Federated identity credentials** instead of stored secrets
- **Environment-specific service principals** with minimal permissions
- **Automatic token exchange** during workflow execution

**Code Example:**
```yaml
# From boot-matrix-plan-apply.yaml
permissions:
  contents: "read"
  id-token: "write"  # Required for OIDC
  
env:
  ARM_CLIENT_ID: ${{ vars.BOOTSTRAP_SUBSCRIPTION_OWNER_APP_ID }}
  ARM_USE_OIDC: true
  ARM_SUBSCRIPTION_ID: ${{ vars.AZ_ROOT_SUBSCRIPTION_ID }}
  ARM_TENANT_ID: ${{ vars.AZ_TENANT_ID }}

steps:
  - name: "Az CLI login"
    uses: azure/login@8c334a195cbb38e46038007b304988d888bf676a
    with:
      client-id: ${{ vars.BOOTSTRAP_SUBSCRIPTION_OWNER_APP_ID }}
      tenant-id: ${{ vars.AZ_TENANT_ID }}
      subscription-id: ${{ vars.AZ_ROOT_SUBSCRIPTION_ID }}
```

**Repository Reference:** [gc-bootstrap-config/.github/workflows/boot-matrix-plan-apply.yaml](gc-bootstrap-config/.github/workflows/boot-matrix-plan-apply.yaml#L15-L50)

**Production Importance:** OIDC eliminates long-lived secrets, reduces attack surface, provides better audit trails, and aligns with zero-trust security principles.

---

**Q8: Explain the matrix build strategy for infrastructure deployments.**

**Answer:** The platform uses GitHub Actions matrix builds to deploy infrastructure across multiple customer organizations simultaneously:
- **Dynamic matrix generation** based on tier configuration
- **Parallel execution** for different customers/environments
- **Dependency management** between deployment layers
- **Rollback capabilities** per customer

**Code Example:**
```yaml
# From boot-matrix-plan-apply.yaml
strategy:
  matrix:
    directory: ${{ fromJson(needs.get-directories.outputs.directories) }}
  fail-fast: false
  max-parallel: 10

concurrency:
  group: bootstrap-"${{ inputs.directory }}"
  cancel-in-progress: false
```

**Repository Reference:** [gc-bootstrap-config/.github/workflows/boot-matrix-plan-apply.yaml](gc-bootstrap-config/.github/workflows/boot-matrix-plan-apply.yaml#L20-L30)

**Production Importance:** Matrix builds enable efficient scaling across multiple tenants while maintaining isolation and allowing for selective rollbacks when issues occur.

---

### Banking Domain & Integrations

**Q9: What banking systems does the platform integrate with and how are they structured?**

**Answer:** The platform integrates with multiple core banking systems through dedicated connectors:

**Banking Systems:**
- **Mambu**: Cloud-native core banking (party, deposit, loan, payment connectors)
- **Flexcube**: Oracle's universal banking platform
- **FIS**: Financial industry solutions
- **Fiservdna**: Core processing platform
- **Silverlake**: Retail banking system

**Connector Structure:**
```yaml
# Each connector has multiple versions and functionalities
- chart: gc-mambu-party-connector
  targetRevision: 1.0.10
  releaseName: party-v0
  
- chart: gc-mambu-deposit-connector
  targetRevision: 1.1.1-SNAPSHOT-6c846f2f
  releaseName: deposit-v0
```

**Repository Reference:** [gc-ecos-applications-live/runtimes/dev/apps/](gc-ecos-applications-live/runtimes/dev/apps/)

**Production Importance:** Standardized connector architecture enables rapid onboarding of new financial institutions while maintaining compliance and ensuring consistent API interfaces across different banking platforms.

---

## ðŸ“™ Advanced Questions

### Architecture & Scalability

**Q10: How would you scale this platform to support 100+ financial institutions?**

**Answer:** Scaling strategy would involve:

**1. Resource Optimization:**
- Implement resource quotas and limits per tenant
- Use Azure spot instances for non-critical workloads
- Implement horizontal pod autoscaling based on banking transaction volumes

**2. Data Partitioning:**
- Tenant-specific namespaces with network policies
- Separate storage accounts per customer tier
- Database sharding by customer ID

**3. Infrastructure Automation:**
- Customer self-service onboarding portal
- Automated tier assignment based on customer size/requirements
- Template-based infrastructure provisioning

**4. Observability at Scale:**
```yaml
# Implement customer-specific monitoring
resources:
  - apiVersion: v1
    kind: Namespace
    metadata:
      name: gc-${customer}-${environment}
      labels:
        customer: ${customer}
        tier: ${tier}
        monitoring.enabled: "true"
```

**Production Importance:** Financial institutions require isolation, compliance, and performance guarantees. Proper scaling ensures regulatory compliance while maintaining cost efficiency and operational simplicity.

---

**Q11: Analyze the potential security vulnerabilities in the current architecture and propose mitigations.**

**Answer:** 

**Identified Vulnerabilities:**

1. **Network Access to Key Vault** (commented out private endpoints):
```terraform
# Current issue: Key vault accessible from internet
#checkov:skip=CKV_AZURE_189: we need to access this vault from TF
# network_acls commented out
```

**Mitigation:**
```terraform
network_acls {
  default_action = "Deny"
  bypass         = "AzureServices"
  ip_rules       = local.allowed_github_runner_ips
  virtual_network_subnet_ids = [azurerm_subnet.terraform_subnet.id]
}
```

2. **Container Image Security:**
- Implement image vulnerability scanning in CI/CD
- Use distroless or minimal base images
- Implement admission controllers for image verification

3. **Secrets Rotation:**
- Implement automated certificate rotation
- Use Azure Key Vault auto-rotation capabilities
- Implement secret scanning in repositories

**Repository Reference:** [gc-infra/shared/keyvault.tf](gc-infra/shared/keyvault.tf#L11-L30)

**Production Importance:** Financial services are prime targets for attackers. Proactive security measures prevent data breaches, regulatory violations, and maintain customer trust.

---

**Q12: How would you implement disaster recovery for this platform?**

**Answer:**

**Multi-Region Strategy:**
```terraform
# Primary region infrastructure
module "primary_region" {
  source = "./modules/region"
  location = "West Europe"
  is_primary = true
  backup_retention_days = 30
}

# DR region infrastructure  
module "dr_region" {
  source = "./modules/region"
  location = "North Europe"
  is_primary = false
  backup_retention_days = 7
}
```

**Data Protection:**
1. **Database Replication**: Geo-redundant backups for banking data
2. **State Management**: Cross-region Terraform state replication
3. **Container Registry**: Multi-region Azure Container Registry replication
4. **GitOps Recovery**: ArgoCD cluster backup and restore procedures

**RTO/RPO Targets:**
- **Critical Banking Services**: RTO < 4 hours, RPO < 15 minutes
- **Non-critical Services**: RTO < 8 hours, RPO < 1 hour

**Production Importance:** Financial institutions have strict availability requirements and regulatory mandates for business continuity. Proper DR ensures compliance and customer service continuity.

---

### Performance & Optimization

**Q13: What performance bottlenecks could occur in this architecture and how would you address them?**

**Answer:**

**Potential Bottlenecks:**

1. **ArgoCD Scale Issues:**
```yaml
# Problem: Single ArgoCD managing 100+ applications
apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: grandcentral
spec:
  # Solution: Implement application sharding
  sourceRepos:
  - '*'
  destinations:
  - namespace: 'gc-*'
    server: https://kubernetes.default.svc
```

**Solutions:**
- Implement ArgoCD ApplicationSets for scalable app management
- Use app-of-apps pattern for hierarchical deployments
- Implement resource-based sharding

2. **Terraform State Locking:**
```bash
# Issue: Terraform state locks causing deployment delays
# Solution: Implement state splitting and parallel execution
terraform workspace select customer-${CUSTOMER_ID}
terraform apply -parallelism=20 -target=module.non_dependent_resources
```

3. **Banking Connector Performance:**
- Implement connection pooling
- Add Redis caching layer
- Use async processing for non-critical operations

**Production Importance:** Performance issues in banking systems directly impact customer transactions and business operations. Proactive optimization prevents service degradation during peak loads.

---

## ðŸš€ Expert/Architect Questions

### System Design & Architecture Decisions

**Q14: Design a multi-tenant isolation strategy that meets banking regulatory requirements.**

**Answer:**

**Regulatory Requirements Analysis:**
- PCI DSS compliance for payment data
- SOX compliance for financial controls
- Regional data residency requirements
- Audit trail requirements

**Isolation Strategy:**

1. **Infrastructure Layer:**
```terraform
# Customer-specific subscriptions for high-isolation tiers
resource "azurerm_subscription" "customer_subscription" {
  count            = var.isolation_tier == "high" ? 1 : 0
  alias            = "customer-${var.customer_id}"
  subscription_name = "GrandCentral-${var.customer_name}"
  billing_scope_id = var.enterprise_agreement_billing_scope
}

# Shared subscription with strict RBAC for standard tiers
resource "azurerm_role_assignment" "customer_rbac" {
  count              = var.isolation_tier == "standard" ? 1 : 0
  scope              = azurerm_resource_group.customer_rg[0].id
  role_definition_id = data.azurerm_role_definition.contributor.id
  principal_id       = azuread_service_principal.customer_sp.object_id
  
  condition = "((ActionMatches{'Microsoft.Resources/subscriptions/resourceGroups/write'}) OR (ActionMatches{'Microsoft.Resources/subscriptions/resourceGroups/delete'})) AND (@Resource[Microsoft.Resources/subscriptions/resourceGroups:name] StringEquals '${var.customer_prefix}*')"
}
```

2. **Kubernetes Layer:**
```yaml
# Network policies for namespace isolation
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: customer-isolation
  namespace: gc-${customer}
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          customer: ${customer}
    - namespaceSelector:
        matchLabels:
          scope: platform
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          scope: platform
  - to: []
    ports:
    - protocol: TCP
      port: 443  # External banking APIs
```

3. **Data Layer:**
```yaml
# Customer-specific storage encryption
apiVersion: v1
kind: Secret
metadata:
  name: customer-encryption-key
  namespace: gc-${customer}
type: Opaque
data:
  key: ${customer_specific_encryption_key}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: banking-connector
spec:
  template:
    spec:
      containers:
      - name: connector
        env:
        - name: DB_ENCRYPTION_KEY
          valueFrom:
            secretKeyRef:
              name: customer-encryption-key
              key: key
        - name: CUSTOMER_ID
          value: ${customer}
```

**Production Importance:** Banking regulations require demonstrable isolation between customer data. This multi-layered approach ensures compliance while maintaining operational efficiency.

---

**Q15: How would you implement a zero-downtime deployment strategy for critical banking services?**

**Answer:**

**Blue-Green Deployment with Banking-Specific Considerations:**

1. **Database Migration Strategy:**
```sql
-- Backward-compatible schema changes only
ALTER TABLE transactions 
ADD COLUMN new_field VARCHAR(255) NULL;

-- Deploy application v2 that can handle both old and new schema
-- After validation, populate new field
UPDATE transactions SET new_field = legacy_computation(old_field) 
WHERE new_field IS NULL;

-- Make field NOT NULL in a subsequent release
```

2. **ArgoCD Rollout Strategy:**
```yaml
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: banking-connector
spec:
  strategy:
    blueGreen:
      activeService: banking-connector-active
      previewService: banking-connector-preview
      autoPromotionEnabled: false  # Manual approval for banking
      prePromotionAnalysis:
        templates:
        - templateName: connectivity-check
        args:
        - name: service-name
          value: banking-connector-preview
      postPromotionAnalysis:
        templates:
        - templateName: transaction-validation
        args:
        - name: duration
          value: 30m  # Extended validation for banking
```

3. **Circuit Breaker Pattern:**
```java
@Component
public class BankingConnectorService {
    
    @CircuitBreaker(name = "mambu-api", fallbackMethod = "fallbackToCache")
    @TimeLimiter(name = "mambu-api")
    @Retry(name = "mambu-api")
    public CompletableFuture<AccountBalance> getAccountBalance(String accountId) {
        return CompletableFuture.supplyAsync(() -> 
            mambuApiClient.getAccountBalance(accountId)
        );
    }
    
    public CompletableFuture<AccountBalance> fallbackToCache(String accountId, Exception ex) {
        return CompletableFuture.supplyAsync(() -> 
            cacheService.getCachedBalance(accountId)
                .orElseThrow(() -> new ServiceUnavailableException("Banking service temporarily unavailable"))
        );
    }
}
```

4. **Feature Flags for Gradual Rollout:**
```yaml
apiVersion: split.io/v1alpha1
kind: Split
metadata:
  name: new-payment-engine
spec:
  treatments:
  - name: control
    weight: 90
  - name: treatment
    weight: 10
  rules:
  - condition: "customer in segment \"beta_customers\""
    treatment: treatment
  - condition: "customer in segment \"high_value_customers\""
    treatment: control
```

**Production Importance:** Banking services cannot afford downtime during business hours. This strategy ensures continuous service availability while allowing for thorough validation of new releases.

---

### Compliance & Governance

**Q16: Design a comprehensive audit and compliance framework for this platform.**

**Answer:**

**Regulatory Framework Implementation:**

1. **Infrastructure Audit Trail:**
```terraform
# All resources must have audit logging enabled
resource "azurerm_monitor_diagnostic_setting" "resource_audit" {
  name               = "audit-${var.resource_name}"
  target_resource_id = var.resource_id
  storage_account_id = azurerm_storage_account.audit_logs.id

  enabled_log {
    category = "AuditEvent"
  }
  
  enabled_log {
    category = "Administrative"
  }

  metric {
    category = "AllMetrics"
  }
}
```

2. **GitOps Audit Trail:**
```yaml
# ArgoCD audit configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: argocd-audit-policy
data:
  policy.yaml: |
    rules:
    - level: RequestResponse
      resources:
      - group: argoproj.io
        resources: ["applications", "appprojects"]
      namespaces: ["argocd"]
    - level: Request
      resources:
      - group: ""
        resources: ["secrets", "configmaps"]
```

3. **Data Governance:**
```python
# Data classification and retention policies
class DataGovernancePolicy:
    def __init__(self):
        self.policies = {
            "customer_data": {
                "encryption": "required",
                "retention_days": 2555,  # 7 years for banking
                "geographic_restriction": True,
                "audit_level": "detailed"
            },
            "transaction_data": {
                "encryption": "required", 
                "retention_days": 3650,  # 10 years for transactions
                "immutable": True,
                "audit_level": "comprehensive"
            }
        }
    
    def apply_policy(self, data_type: str, resource: dict):
        policy = self.policies.get(data_type)
        if not policy:
            raise ValueError(f"No policy defined for {data_type}")
            
        return {
            "resource": resource,
            "compliance_metadata": {
                "classification": data_type,
                "retention_end": datetime.now() + timedelta(days=policy["retention_days"]),
                "encryption_required": policy["encryption"],
                "audit_enabled": True
            }
        }
```

4. **Compliance Validation:**
```yaml
# Kyverno policies for regulatory compliance
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: pci-dss-compliance
spec:
  background: false
  validationFailureAction: enforce
  rules:
  - name: require-encryption
    match:
      resources:
        kinds:
        - Secret
        - ConfigMap
    validate:
      message: "PCI DSS requires encryption for sensitive data"
      pattern:
        metadata:
          annotations:
            encryption.enabled: "true"
  - name: network-segmentation
    match:
      resources:
        kinds:
        - Namespace
    validate:
      message: "Banking namespaces must have network policies"
      deny:
        conditions:
        - key: "{{ request.object.metadata.labels.tier || '' }}"
          operator: Equals
          value: banking
        - key: "{{ query(@, 'spec.networkPolicies[*].name') | length(@) }}"
          operator: LessThan
          value: 1
```

**Production Importance:** Financial institutions face strict regulatory oversight. Comprehensive audit capabilities ensure compliance, facilitate regulatory examinations, and provide forensic capabilities for security incidents.

---

## ðŸ” Code Review & Refactoring Questions

### Security Code Review

**Q17: Review this Terraform configuration and identify security issues:**

```terraform
resource "azurerm_key_vault" "kv_shared" {
  #checkov:skip=CKV_AZURE_189: we need to access this vault from TF
  location                        = var.location
  name                            = local.shared_keyvault_name
  resource_group_name             = local.shared_resourcegroup
  sku_name                        = "standard"
  tenant_id                       = local.tenant_id
  enabled_for_disk_encryption     = false
  enabled_for_deployment          = false
  enable_rbac_authorization       = true
  soft_delete_retention_days      = 7
  /*
  network_acls {
    default_action = "Deny"
    bypass         = "AzureServices"
    ip_rules       = local.allowed_public_ips
  }
  */
}
```

**Answer:**

**Security Issues Identified:**

1. **Network Access Control Disabled:**
   - Key Vault is accessible from any public IP
   - Network ACLs are commented out

2. **Insufficient Soft Delete Retention:**
   - 7 days is too short for critical banking infrastructure
   - Should be 30-90 days minimum

3. **Missing Purge Protection:**
   - No `purge_protection_enabled = true` 
   - Critical for banking compliance

**Improved Configuration:**
```terraform
resource "azurerm_key_vault" "kv_shared" {
  location                        = var.location
  name                            = local.shared_keyvault_name
  resource_group_name             = local.shared_resourcegroup
  sku_name                        = "premium"  # Hardware security modules
  tenant_id                       = local.tenant_id
  enabled_for_disk_encryption     = false
  enabled_for_deployment          = false
  enable_rbac_authorization       = true
  soft_delete_retention_days      = 90
  purge_protection_enabled        = true  # Added
  public_network_access_enabled   = false  # Added

  network_acls {
    default_action = "Deny"
    bypass         = "AzureServices"
    ip_rules       = local.github_runner_ips
    virtual_network_subnet_ids = [
      azurerm_subnet.terraform_subnet.id,
      azurerm_subnet.aks_subnet.id
    ]
  }
}

# Add private endpoint
resource "azurerm_private_endpoint" "kv_shared" {
  name                = "pe-kv-shared"
  location            = var.location
  resource_group_name = local.shared_resourcegroup
  subnet_id           = azurerm_subnet.private_endpoint_subnet.id
  
  private_service_connection {
    name                           = "pe-kv-shared"
    private_connection_resource_id = azurerm_key_vault.kv_shared.id
    is_manual_connection           = false
    subresource_names              = ["vault"]
  }
}
```

**Repository Reference:** [gc-infra/shared/keyvault.tf](gc-infra/shared/keyvault.tf#L11-L40)

---

### Architecture Refactoring

**Q18: The current ArgoCD application structure has multiple connectors in a single application. How would you refactor this for better maintainability?**

**Current Structure Issue:**
```yaml
# Single application with multiple connectors - hard to manage
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: gc-mambu
spec:
  sources:
    - chart: gc-mambu-party-connector
      targetRevision: 1.0.10
    - chart: gc-mambu-deposit-connector  
      targetRevision: 1.1.1-SNAPSHOT-6c846f2f
    - chart: gc-mambu-loan-connector
      targetRevision: 1.1.1-SNAPSHOT-ce50fa19
    # ... 8 more connectors
```

**Refactored Solution:**

**1. ApplicationSet Pattern:**
```yaml
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: mambu-connectors
spec:
  generators:
  - list:
      elements:
      - connector: party
        version: "1.0.10"
        config: party-v0
        replicas: 3
      - connector: deposit
        version: "1.1.1-SNAPSHOT-6c846f2f"  
        config: deposit-v0
        replicas: 2
      - connector: loan
        version: "1.1.1-SNAPSHOT-ce50fa19"
        config: loan-v0
        replicas: 2
  template:
    metadata:
      name: 'mambu-{{connector}}-{{config}}'
      namespace: argocd
    spec:
      project: grandcentral
      source:
        repoURL: crecos493.azurecr.io/charts
        chart: 'gc-mambu-{{connector}}-connector'
        targetRevision: '{{version}}'
        helm:
          releaseName: '{{config}}'
          parameters:
          - name: replicaCount
            value: '{{replicas}}'
          valueFiles:
          - '$apps-live/runtimes/{{env}}/values/platform-resource-management/platform-resource-management.yaml'
          - '$apps-live/runtimes/{{env}}/values/gc-mambu/{{config}}.values.yaml'
      destination:
        server: https://kubernetes.default.svc
        namespace: gc-mambu
```

**2. App-of-Apps Pattern:**
```yaml
# Root application
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: banking-platform
spec:
  source:
    repoURL: git@github.com:bb-ecos-ecos/gc-ecos-applications-live.git
    path: runtimes/{{env}}/platform-apps
    targetRevision: main
  destination:
    server: https://kubernetes.default.svc
    namespace: argocd

---
# Platform apps directory structure:
# runtimes/dev/platform-apps/
# â”œâ”€â”€ mambu-connectors.yaml
# â”œâ”€â”€ flexcube-connectors.yaml 
# â”œâ”€â”€ fis-connectors.yaml
# â””â”€â”€ platform-services.yaml
```

**Benefits:**
- **Independent versioning** per connector
- **Selective rollbacks** without affecting other services
- **Easier testing** and validation
- **Better observability** per connector
- **Simplified CI/CD** pipelines

---

## ðŸ§  Scenario-Based Questions

### Production Incident Scenarios

**Q19: At 9 AM on Monday morning, multiple customers report that their banking transactions are failing. Your monitoring shows that the Mambu party connector is returning HTTP 503 errors. Walk through your incident response process.**

**Answer:**

**Immediate Response (0-5 minutes):**

1. **Acknowledge and Escalate:**
```bash
# Check ArgoCD application status
kubectl get applications -n argocd | grep mambu
kubectl describe application gc-mambu -n argocd

# Check pod status
kubectl get pods -n gc-mambu -l app=party-connector
kubectl logs -n gc-mambu deployment/mambu-party-v0 --tail=100
```

2. **Implement Circuit Breaker:**
```yaml
# Immediate mitigation - route to fallback service
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: mambu-party-circuit-breaker
spec:
  host: mambu-party-service
  trafficPolicy:
    outlierDetection:
      consecutiveErrors: 1
      interval: 30s
      baseEjectionTime: 30s
```

**Investigation (5-15 minutes):**

3. **Check External Dependencies:**
```bash
# Test Mambu API connectivity
kubectl run debug-pod --rm -it --image=nicolaka/netshoot -- \
  curl -v https://mambu-sandbox.mambu.com/api/health

# Check Azure Service Bus (for async processing)
az servicebus namespace show --name gc-servicebus --resource-group rg-shared
```

4. **Review Recent Changes:**
```bash
# Check recent ArgoCD syncs
kubectl get events -n gc-mambu --sort-by='.lastTimestamp' | head -20

# Review recent deployments
git log --oneline --since="24 hours ago" -- runtimes/*/values/gc-mambu/
```

**Resolution (15-60 minutes):**

5. **Database Connection Issues (Common Cause):**
```sql
-- Check active connections to Mambu database
SELECT datname, numbackends, xact_commit, xact_rollback 
FROM pg_stat_database 
WHERE datname = 'mambu_integration';

-- Check for blocking queries
SELECT blocked_locks.pid AS blocked_pid,
       blocking_locks.pid AS blocking_pid,
       blocked_activity.query AS blocked_query
FROM pg_catalog.pg_locks blocked_locks
JOIN pg_catalog.pg_stat_activity blocked_activity 
  ON blocked_activity.pid = blocked_locks.pid;
```

6. **Scale Out Strategy:**
```yaml
# Temporarily increase replicas
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mambu-party-v0
spec:
  replicas: 6  # Increased from 3
  template:
    spec:
      containers:
      - name: party-connector
        resources:
          requests:
            memory: "1Gi" 
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
```

**Communication:**
- **Customer notification** within 15 minutes
- **Regular updates** every 30 minutes
- **Detailed postmortem** within 24 hours

**Production Importance:** Banking systems have zero tolerance for downtime during business hours. Structured incident response ensures rapid resolution and maintains customer trust.

---

**Q20: You notice that the Terraform state files are becoming corrupted due to concurrent runs. How would you implement state management best practices?**

**Answer:**

**Current Problem Analysis:**
```bash
# Common symptoms of state corruption
terraform plan  # Shows resources that shouldn't be recreated
terraform validate  # May pass but state is inconsistent
```

**Solution Implementation:**

**1. Enhanced State Locking:**
```terraform
# Improved backend configuration
terraform {
  backend "azurerm" {
    resource_group_name  = "rg-tfstate-${var.environment}"
    storage_account_name = "stgctfstate${random_id.suffix.hex}"
    container_name       = "tfstate"
    key                  = "terraform-${var.workspace}.tfstate"
    use_azuread_auth     = true
    
    # Enhanced locking
    snapshot              = true
    
    # Access tier optimization
    access_tier          = "Hot"
  }
}

# State storage account with enhanced security
resource "azurerm_storage_account" "tfstate" {
  name                      = "stgctfstate${random_id.suffix.hex}"
  resource_group_name       = azurerm_resource_group.tfstate.name
  location                  = var.location
  account_tier              = "Standard"
  account_replication_type  = "ZRS"  # Zone redundancy
  
  # Enhanced security
  allow_nested_items_to_be_public = false
  shared_access_key_enabled       = false
  public_network_access_enabled   = false
  
  # Versioning for state history
  versioning_enabled = true
  
  # Legal hold for compliance
  blob_properties {
    versioning_enabled = true
    delete_retention_policy {
      days = 30
    }
    container_delete_retention_policy {
      days = 30
    }
  }
}
```

**2. Workspace-Based Isolation:**
```bash
#!/bin/bash
# Improved deployment script
set -euo pipefail

WORKSPACE="${CUSTOMER_ID}-${ENVIRONMENT}"
LOCK_TABLE="tfstate-locks"

# Ensure workspace isolation
terraform workspace select "$WORKSPACE" || terraform workspace new "$WORKSPACE"

# Check for existing locks before proceeding
if az storage blob exists \
  --account-name "$STORAGE_ACCOUNT" \
  --container-name "$CONTAINER" \
  --name "${WORKSPACE}.tflock" \
  --auth-mode login; then
  
  # Check lock age and break if stale
  LOCK_AGE=$(az storage blob show \
    --account-name "$STORAGE_ACCOUNT" \
    --container-name "$CONTAINER" \
    --name "${WORKSPACE}.tflock" \
    --query 'properties.lastModified' \
    --output tsv)
  
  if [[ $(date -d "$LOCK_AGE" +%s) -lt $(date -d "2 hours ago" +%s) ]]; then
    echo "Breaking stale lock older than 2 hours"
    terraform force-unlock -force
  else
    echo "Active lock detected, failing build"
    exit 1
  fi
fi

# Proceed with deployment
terraform plan -lock-timeout=300s -out=tfplan
terraform apply tfplan
```

**3. State Backup and Recovery:**
```yaml
# GitHub Actions workflow with state backup
name: Terraform with State Protection
jobs:
  deploy:
    steps:
    - name: Backup current state
      run: |
        # Download current state
        terraform state pull > backup-$(date +%Y%m%d-%H%M%S).tfstate
        
        # Upload to backup storage
        az storage blob upload \
          --file backup-*.tfstate \
          --container-name state-backups \
          --name "${WORKSPACE}/$(date +%Y/%m/%d)/backup-$(date +%H%M%S).tfstate"
    
    - name: Plan with lock timeout
      run: |
        terraform plan \
          -lock-timeout=10m \
          -out=tfplan \
          -var-file="${WORKSPACE}.tfvars.json"
    
    - name: Apply with automatic rollback
      run: |
        if ! terraform apply tfplan; then
          echo "Apply failed, restoring from backup"
          LATEST_BACKUP=$(az storage blob list \
            --container-name state-backups \
            --prefix "${WORKSPACE}" \
            --query 'sort_by([].{name:name,modified:properties.lastModified}, &modified)[-1].name' \
            --output tsv)
          
          az storage blob download \
            --container-name state-backups \
            --name "$LATEST_BACKUP" \
            --file restore.tfstate
          
          terraform state push restore.tfstate
          exit 1
        fi
```

**4. Monitoring and Alerting:**
```python
# State health monitoring
import json
import subprocess
from datetime import datetime, timezone

def check_state_health():
    try:
        # Get state information
        state_raw = subprocess.check_output(['terraform', 'state', 'list'])
        resources = state_raw.decode('utf-8').strip().split('\n')
        
        # Check for drift
        plan_output = subprocess.check_output(['terraform', 'plan', '-detailed-exitcode'])
        
        health_status = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'resource_count': len(resources),
            'drift_detected': plan_output != 0,
            'lock_status': check_lock_status()
        }
        
        return health_status
        
    except subprocess.CalledProcessError as e:
        return {'error': str(e), 'status': 'unhealthy'}

def check_lock_status():
    # Implementation to check Azure blob lock status
    pass
```

**Production Importance:** State corruption can lead to infrastructure outages, resource duplication costs, and compliance violations. Robust state management ensures infrastructure reliability and reduces operational risk.

---

## ðŸ›  Debugging & Production Failure Questions

### Kubernetes Debugging

**Q21: Customers report intermittent timeouts when making banking API calls through the platform. Your APM shows 50% of requests to the Mambu connector are taking >30 seconds. Debug this issue.**

**Answer:**

**Initial Investigation:**

1. **Application-Level Analysis:**
```bash
# Check pod resource usage
kubectl top pods -n gc-mambu
kubectl describe pod $(kubectl get pods -n gc-mambu -l app=mambu-party-connector -o name | head -1)

# Check for pending connections
kubectl exec -n gc-mambu deployment/mambu-party-v0 -- netstat -an | grep :8080
```

2. **Network-Level Debugging:**
```bash
# Check DNS resolution times
kubectl run dns-debug --rm -it --image=nicolaka/netshoot -- \
  nslookup mambu-sandbox.mambu.com

# Test external connectivity with timing
kubectl exec -n gc-mambu deployment/mambu-party-v0 -- \
  curl -w "@curl-format.txt" -o /dev/null -s "https://mambu-sandbox.mambu.com/api/health"

# curl-format.txt content:
#      time_namelookup:  %{time_namelookup}\n
#         time_connect:  %{time_connect}\n
#      time_appconnect:  %{time_appconnect}\n
#         time_total:    %{time_total}\n
```

**Common Root Causes & Solutions:**

3. **Database Connection Pool Exhaustion:**
```yaml
# Check current pool configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: mambu-connector-config
data:
  application.yml: |
    spring:
      datasource:
        # Problem: Pool too small
        hikari:
          maximum-pool-size: 5  # Increase to 20
          connection-timeout: 30000
          idle-timeout: 600000
          max-lifetime: 1800000
          leak-detection-threshold: 60000
        
    # Add connection validation
    jpa:
      properties:
        hibernate:
          connection:
            provider_disables_autocommit: true
```

4. **Memory Pressure Analysis:**
```bash
# Check JVM metrics inside container
kubectl exec -n gc-mambu deployment/mambu-party-v0 -- \
  jcmd 1 VM.info

# Check for GC issues
kubectl exec -n gc-mambu deployment/mambu-party-v0 -- \
  jcmd 1 GC.run_finalization

# Review JVM settings
kubectl get deployment mambu-party-v0 -n gc-mambu -o yaml | grep -A 10 -B 10 JAVA_OPTS
```

5. **External API Rate Limiting:**
```java
// Implement exponential backoff
@Component
public class MambuApiClient {
    
    @Retryable(value = {HttpServerErrorException.class}, 
               backoff = @Backoff(delay = 1000, multiplier = 2, maxDelay = 30000))
    public ResponseEntity<Account> getAccount(String accountId) {
        return restTemplate.getForEntity("/accounts/" + accountId, Account.class);
    }
    
    @EventListener
    public void handleRateLimitExceeded(RateLimitExceededException event) {
        // Implement circuit breaker logic
        circuitBreakerRegistry.circuitBreaker("mambu-api").transitionToOpenState();
        
        // Schedule retry after rate limit reset
        taskScheduler.schedule(() -> {
            circuitBreakerRegistry.circuitBreaker("mambu-api").transitionToClosedState();
        }, Instant.now().plusSeconds(event.getRetryAfterSeconds()));
    }
}
```

6. **Istio Service Mesh Debugging:**
```bash
# Check Envoy proxy stats
kubectl exec -n gc-mambu deployment/mambu-party-v0 -c istio-proxy -- \
  curl localhost:15000/stats | grep "cluster.outbound.*mambu.*pending"

# Check circuit breaker status
kubectl exec -n gc-mambu deployment/mambu-party-v0 -c istio-proxy -- \
  curl localhost:15000/clusters | grep mambu

# Review Istio configuration
kubectl get destinationrule mambu-party -n gc-mambu -o yaml
```

**Performance Optimization:**

7. **Implement Request Queuing:**
```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: mambu-party-queue
spec:
  http:
  - match:
    - headers:
        priority:
          exact: "high"
    route:
    - destination:
        host: mambu-party-service
        subset: high-priority
    timeout: 10s
  - route:
    - destination:
        host: mambu-party-service
        subset: standard
    timeout: 30s

---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: mambu-party-subsets
spec:
  host: mambu-party-service
  subsets:
  - name: high-priority
    labels:
      version: v2
    trafficPolicy:
      connectionPool:
        tcp:
          maxConnections: 20
        http:
          http1MaxPendingRequests: 5
          maxRequestsPerConnection: 2
  - name: standard
    labels:
      version: v0
    trafficPolicy:
      connectionPool:
        tcp:
          maxConnections: 10
        http:
          http1MaxPendingRequests: 100
```

**Monitoring Implementation:**
```yaml
# Enhanced monitoring
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules
data:
  banking-sla.yml: |
    groups:
    - name: banking.response.time
      rules:
      - alert: BankingAPISlowResponse
        expr: |
          histogram_quantile(0.95, 
            rate(http_request_duration_seconds_bucket{service="mambu-party"}[5m])
          ) > 10
        for: 2m
        labels:
          severity: warning
          customer_impact: high
        annotations:
          summary: "Banking API response time degraded"
          description: "95th percentile response time is {{ $value }}s"
```

**Production Importance:** Banking API performance directly affects customer experience and transaction processing. Systematic debugging prevents revenue loss and maintains regulatory compliance.

---

### Infrastructure Failure Recovery

**Q22: During a routine Kubernetes upgrade, the ArgoCD server becomes unresponsive and all banking applications show "OutOfSync" status. Walk through your recovery process.**

**Answer:**

**Immediate Assessment (0-10 minutes):**

1. **Verify Banking Services Status:**
```bash
# Check if applications are still running despite ArgoCD issues
kubectl get pods -n gc-mambu -o wide
kubectl get pods -n gc-flexcube -o wide
kubectl get pods -n gc-fis -o wide

# Check service endpoints
kubectl get endpoints -A | grep -E "(mambu|flexcube|fis)"

# Verify external connectivity
for service in mambu-party flexcube-core fis-payment; do
  kubectl run test-$service --rm -it --image=curlimages/curl -- \
    curl -f "http://$service.default.svc.cluster.local:8080/health"
done
```

2. **ArgoCD Diagnostics:**
```bash
# Check ArgoCD pods status
kubectl get pods -n argocd
kubectl logs -n argocd deployment/argocd-server --tail=50
kubectl logs -n argocd deployment/argocd-application-controller --tail=50

# Check ArgoCD database connectivity
kubectl exec -n argocd deployment/argocd-server -- \
  argocd cluster list --server localhost:8080
```

**Recovery Strategy:**

3. **ArgoCD Server Recovery:**
```bash
# Scale down ArgoCD components
kubectl scale deployment argocd-server --replicas=0 -n argocd
kubectl scale deployment argocd-application-controller --replicas=0 -n argocd

# Check for PVC corruption
kubectl get pvc -n argocd
kubectl describe pvc argocd-server -n argocd

# Restart with fresh configuration
kubectl delete pod -n argocd -l app.kubernetes.io/name=argocd-server
kubectl delete pod -n argocd -l app.kubernetes.io/name=argocd-application-controller

# Scale back up
kubectl scale deployment argocd-server --replicas=1 -n argocd
kubectl scale deployment argocd-application-controller --replicas=1 -n argocd
```

4. **Database Backup Recovery:**
```bash
# If ArgoCD database is corrupted, restore from backup
kubectl create job argocd-restore --from=cronjob/argocd-backup

# Verify backup restoration
kubectl logs job/argocd-restore -n argocd

# Alternative: Re-import applications from Git
kubectl apply -f - <<EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: argocd-app-restore
  namespace: argocd
spec:
  template:
    spec:
      containers:
      - name: restore
        image: argoproj/argocd:v2.8.0
        command:
        - /bin/bash
        - -c
        - |
          argocd login argocd-server:443 --username admin --password $ARGOCD_PASSWORD --insecure
          
          # Re-create applications from Git repository
          find /apps -name "*.yaml" | xargs -I {} argocd app create -f {}
        env:
        - name: ARGOCD_PASSWORD
          valueFrom:
            secretKeyRef:
              name: argocd-initial-admin-secret
              key: password
        volumeMounts:
        - name: app-configs
          mountPath: /apps
      volumes:
      - name: app-configs
        git:
          repository: git@github.com:bb-ecos-ecos/gc-ecos-applications-live.git
          revision: main
      restartPolicy: OnFailure
EOF
```

5. **Manual Application Sync:**
```bash
# If ArgoCD remains unstable, manually ensure application state
NAMESPACES=("gc-mambu" "gc-flexcube" "gc-fis")

for ns in "${NAMESPACES[@]}"; do
  echo "Checking namespace: $ns"
  
  # Get desired state from Git
  git clone --depth 1 git@github.com:bb-ecos-ecos/gc-ecos-applications-live.git /tmp/apps
  
  # Apply configurations manually
  find /tmp/apps/runtimes/prod/values/$ns -name "*.yaml" | while read file; do
    echo "Applying $file"
    kubectl apply -f "$file" || echo "Failed to apply $file"
  done
  
  # Verify deployments are running
  kubectl rollout status deployment -n $ns --timeout=300s
done
```

6. **Application Health Validation:**
```bash
# Comprehensive health check script
#!/bin/bash
check_banking_health() {
  local namespace=$1
  local service=$2
  
  echo "Checking $service in $namespace"
  
  # Check pod readiness
  local ready_pods=$(kubectl get pods -n $namespace -l app=$service \
    -o jsonpath='{.items[*].status.conditions[?(@.type=="Ready")].status}' | \
    grep -o "True" | wc -l)
  
  local total_pods=$(kubectl get pods -n $namespace -l app=$service --no-headers | wc -l)
  
  if [ "$ready_pods" -eq "$total_pods" ] && [ "$total_pods" -gt 0 ]; then
    echo "âœ“ $service: $ready_pods/$total_pods pods ready"
  else
    echo "âœ— $service: $ready_pods/$total_pods pods ready"
    return 1
  fi
  
  # Check service connectivity
  if kubectl run test-connectivity-$service --rm -i --image=curlimages/curl -- \
     curl -f -s "http://$service.$namespace.svc.cluster.local:8080/health" > /dev/null; then
    echo "âœ“ $service: Health check passed"
  else
    echo "âœ— $service: Health check failed"
    return 1
  fi
}

# Check all critical services
SERVICES=(
  "gc-mambu:mambu-party-connector"
  "gc-mambu:mambu-deposit-connector" 
  "gc-flexcube:flexcube-core"
  "gc-fis:fis-payment-connector"
)

for service_info in "${SERVICES[@]}"; do
  IFS=':' read -r namespace service <<< "$service_info"
  check_banking_health "$namespace" "$service"
done
```

**Prevention Measures:**

7. **Implement ArgoCD High Availability:**
```yaml
# Enhanced ArgoCD configuration
apiVersion: argoproj.io/v1alpha1
kind: ArgoCD
metadata:
  name: argocd
spec:
  ha:
    enabled: true
    
  # External database for reliability
  postgres:
    host: postgres.argocd.svc.cluster.local
    database: argocd
    
  # Backup configuration  
  backup:
    enabled: true
    schedule: "0 */6 * * *"
    destination:
      storageAccount: argocdbkup
      container: backups
      
  # Split components for isolation
  controller:
    replicas: 2
    resources:
      requests:
        memory: "2Gi"
        cpu: "1000m"
        
  server:
    replicas: 2
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
```

**Production Importance:** ArgoCD failure can disrupt all deployments in a banking environment. Rapid recovery procedures ensure minimal impact on customer-facing services and maintain regulatory compliance.

---

## Technical Implementation Summary

This interview question bank covers the complete spectrum of the Grand Central platform's technical implementations:

### Key Technologies Validated:
- **Azure Infrastructure**: Terraform/OpenTofu, multi-tier deployments, RBAC, Key Vault
- **Kubernetes**: ArgoCD GitOps, multi-source applications, policy enforcement
- **Banking Integrations**: Mambu, Flexcube, FIS connectors, financial compliance
- **Security**: OIDC authentication, secrets management, network policies
- **CI/CD**: GitHub Actions, matrix builds, automated testing
- **Observability**: Monitoring, alerting, compliance validation

### Interview Effectiveness:
- **Progressive Difficulty**: From basic configuration understanding to complex architectural decisions
- **Real-World Scenarios**: Based on actual production challenges in financial technology
- **Code-Focused**: Includes actual configuration files and implementation patterns
- **Production-Ready**: Emphasizes compliance, security, and operational excellence

### Candidate Assessment:
- **Junior Level**: Basic understanding of Infrastructure as Code and Kubernetes
- **Mid Level**: Security practices, CI/CD implementation, troubleshooting skills
- **Senior Level**: Architecture decisions, performance optimization, incident response
- **Expert Level**: System design, compliance frameworks, complex debugging

The questions are directly derived from the actual repository implementations, ensuring candidates are assessed on real-world skills needed for this specific financial technology platform.
